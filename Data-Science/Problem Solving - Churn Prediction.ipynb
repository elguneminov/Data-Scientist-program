{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "* About Data - Rental bike usage data\n",
    "* We wanted to predict churn (customers leaving your product)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data = pd.read_csv('https://raw.githubusercontent.com/edyoda/data-science-complete-tutorial/master/Data/churn.csv.txt', parse_dates=['last_trip_date','signup_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 12 columns):\n",
      "avg_dist                  50000 non-null float64\n",
      "avg_rating_by_driver      49799 non-null float64\n",
      "avg_rating_of_driver      41878 non-null float64\n",
      "avg_surge                 50000 non-null float64\n",
      "city                      50000 non-null object\n",
      "last_trip_date            50000 non-null datetime64[ns]\n",
      "phone                     49604 non-null object\n",
      "signup_date               50000 non-null datetime64[ns]\n",
      "surge_pct                 50000 non-null float64\n",
      "trips_in_first_30_days    50000 non-null int64\n",
      "luxury_car_user           50000 non-null bool\n",
      "weekday_pct               50000 non-null float64\n",
      "dtypes: bool(1), datetime64[ns](2), float64(6), int64(1), object(2)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "churn_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there any churn column in data?\n",
    "* Many times, target column is directly not available.\n",
    "* It has to be derived from feature columns\n",
    "* From the data, we need to identify the date on which the data was downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2014-07-01 00:00:00')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_data.last_trip_date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = churn_data.last_trip_date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_date = last_date - datetime.timedelta(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If an user didn't come after cutoff date he/she is considered as churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data['churn'] = churn_data.last_trip_date.map(lambda d: 'Not Churn' if d > cutoff_date else 'Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_dist</th>\n",
       "      <th>avg_rating_by_driver</th>\n",
       "      <th>avg_rating_of_driver</th>\n",
       "      <th>avg_surge</th>\n",
       "      <th>city</th>\n",
       "      <th>last_trip_date</th>\n",
       "      <th>phone</th>\n",
       "      <th>signup_date</th>\n",
       "      <th>surge_pct</th>\n",
       "      <th>trips_in_first_30_days</th>\n",
       "      <th>luxury_car_user</th>\n",
       "      <th>weekday_pct</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34568</th>\n",
       "      <td>3.12</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Astapor</td>\n",
       "      <td>2014-05-18</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>2014-01-26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Churn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35933</th>\n",
       "      <td>19.36</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>King's Landing</td>\n",
       "      <td>2014-06-22</td>\n",
       "      <td>Android</td>\n",
       "      <td>2014-01-25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Not Churn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13357</th>\n",
       "      <td>1.91</td>\n",
       "      <td>4.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.05</td>\n",
       "      <td>Winterfell</td>\n",
       "      <td>2014-06-23</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>2014-01-10</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>60.0</td>\n",
       "      <td>Not Churn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42151</th>\n",
       "      <td>3.47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Astapor</td>\n",
       "      <td>2014-02-01</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>2014-01-30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Churn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>3.53</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.20</td>\n",
       "      <td>Astapor</td>\n",
       "      <td>2014-05-17</td>\n",
       "      <td>Android</td>\n",
       "      <td>2014-01-15</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Churn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       avg_dist  avg_rating_by_driver  avg_rating_of_driver  avg_surge  \\\n",
       "34568      3.12                   5.0                   NaN       1.00   \n",
       "35933     19.36                   5.0                   NaN       1.00   \n",
       "13357      1.91                   4.6                   5.0       1.05   \n",
       "42151      3.47                   5.0                   5.0       1.00   \n",
       "1728       3.53                   5.0                   4.7       1.20   \n",
       "\n",
       "                 city last_trip_date    phone signup_date  surge_pct  \\\n",
       "34568         Astapor     2014-05-18   iPhone  2014-01-26        0.0   \n",
       "35933  King's Landing     2014-06-22  Android  2014-01-25        0.0   \n",
       "13357      Winterfell     2014-06-23   iPhone  2014-01-10       20.0   \n",
       "42151         Astapor     2014-02-01   iPhone  2014-01-30        0.0   \n",
       "1728          Astapor     2014-05-17  Android  2014-01-15       20.0   \n",
       "\n",
       "       trips_in_first_30_days  luxury_car_user  weekday_pct      churn  \n",
       "34568                       1            False         20.0      Churn  \n",
       "35933                       1            False          0.0  Not Churn  \n",
       "13357                       0            False         60.0  Not Churn  \n",
       "42151                       2             True         50.0      Churn  \n",
       "1728                        1            False         80.0      Churn  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avg_dist                         float64\n",
       "avg_rating_by_driver             float64\n",
       "avg_rating_of_driver             float64\n",
       "avg_surge                        float64\n",
       "city                              object\n",
       "last_trip_date            datetime64[ns]\n",
       "phone                             object\n",
       "signup_date               datetime64[ns]\n",
       "surge_pct                        float64\n",
       "trips_in_first_30_days             int64\n",
       "luxury_car_user                     bool\n",
       "weekday_pct                      float64\n",
       "churn                             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 13 columns):\n",
      "avg_dist                  50000 non-null float64\n",
      "avg_rating_by_driver      49799 non-null float64\n",
      "avg_rating_of_driver      41878 non-null float64\n",
      "avg_surge                 50000 non-null float64\n",
      "city                      50000 non-null object\n",
      "last_trip_date            50000 non-null datetime64[ns]\n",
      "phone                     49604 non-null object\n",
      "signup_date               50000 non-null datetime64[ns]\n",
      "surge_pct                 50000 non-null float64\n",
      "trips_in_first_30_days    50000 non-null int64\n",
      "luxury_car_user           50000 non-null bool\n",
      "weekday_pct               50000 non-null float64\n",
      "churn                     50000 non-null object\n",
      "dtypes: bool(1), datetime64[ns](2), float64(6), int64(1), object(3)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "churn_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_churn_data = churn_data.select_dtypes(include=['float','bool']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 7 columns):\n",
      "avg_dist                50000 non-null float64\n",
      "avg_rating_by_driver    49799 non-null float64\n",
      "avg_rating_of_driver    41878 non-null float64\n",
      "avg_surge               50000 non-null float64\n",
      "surge_pct               50000 non-null float64\n",
      "luxury_car_user         50000 non-null bool\n",
      "weekday_pct             50000 non-null float64\n",
      "dtypes: bool(1), float64(6)\n",
      "memory usage: 2.3 MB\n"
     ]
    }
   ],
   "source": [
    "float_churn_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_pipeline = make_pipeline(SimpleImputer(strategy='median'),MinMaxScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_churn_data = churn_data[['city','phone']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      "city     50000 non-null object\n",
      "phone    49604 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "cat_churn_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline = make_pipeline(SimpleImputer(strategy='most_frequent'), OrdinalEncoder(), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data['subscription_days'] = churn_data.last_trip_date - churn_data.signup_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* timedelta to days conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data['subscription_days'] = churn_data.subscription_days.dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_churn_data = churn_data.select_dtypes(include=['int'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      "trips_in_first_30_days    50000 non-null int64\n",
      "subscription_days         50000 non-null int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 781.4 KB\n"
     ]
    }
   ],
   "source": [
    "int_churn_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_pipeline = make_pipeline(MinMaxScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "    (int_pipeline, int_churn_data.columns),\n",
    "    (cat_pipeline, cat_churn_data.columns),\n",
    "    (float_pipeline, float_churn_data.columns),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Churn        31690\n",
       "Not Churn    18310\n",
       "Name: churn, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_data.churn.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(preprocessor,RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(churn_data.drop(columns=['churn']), churn_data.churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awantik/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('columntransformer',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('pipeline-1',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('minmaxscaler',\n",
       "                                                                   MinMaxScaler(copy=True,\n",
       "                                                                                feature_range=(0,\n",
       "                                                                                               1)))],\n",
       "                                                           verbose=False),\n",
       "                                                  Index(['trips_in_first_30_days', 'subscription_days'], dtype='object')),\n",
       "                                                 ('pip...\n",
       "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                        criterion='gini', max_depth=None,\n",
       "                                        max_features='auto',\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=10, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9544"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Not Churn', 'Not Churn', 'Churn', ..., 'Not Churn', 'Not Churn',\n",
       "       'Churn'], dtype=object)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7611,  263],\n",
       "       [ 307, 4319]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pred=y_pred, y_true=testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipeline, param_grid={'randomforestclassifier__n_estimators':[100,300]},cv=5, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('columntransformer',\n",
       "                                        ColumnTransformer(n_jobs=None,\n",
       "                                                          remainder='drop',\n",
       "                                                          sparse_threshold=0.3,\n",
       "                                                          transformer_weights=None,\n",
       "                                                          transformers=[('pipeline-1',\n",
       "                                                                         Pipeline(memory=None,\n",
       "                                                                                  steps=[('minmaxscaler',\n",
       "                                                                                          MinMaxScaler(copy=True,\n",
       "                                                                                                       feature_range=(0,\n",
       "                                                                                                                      1)))],\n",
       "                                                                                  verbose=False),\n",
       "                                                                         Index(['trips_i...\n",
       "                                                               min_impurity_split=None,\n",
       "                                                               min_samples_leaf=1,\n",
       "                                                               min_samples_split=2,\n",
       "                                                               min_weight_fraction_leaf=0.0,\n",
       "                                                               n_estimators=10,\n",
       "                                                               n_jobs=None,\n",
       "                                                               oob_score=False,\n",
       "                                                               random_state=None,\n",
       "                                                               verbose=0,\n",
       "                                                               warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=4,\n",
       "             param_grid={'randomforestclassifier__n_estimators': [100, 300]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9592266666666667"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestclassifier__n_estimators': 100}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(preprocessor,GaussianNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('columntransformer',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('pipeline-1',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('minmaxscaler',\n",
       "                                                                   MinMaxScaler(copy=True,\n",
       "                                                                                feature_range=(0,\n",
       "                                                                                               1)))],\n",
       "                                                           verbose=False),\n",
       "                                                  Index(['trips_in_first_30_days', 'subscription_days'], dtype='object')),\n",
       "                                                 ('pip...\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='median',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('minmaxscaler',\n",
       "                                                                   MinMaxScaler(copy=True,\n",
       "                                                                                feature_range=(0,\n",
       "                                                                                               1)))],\n",
       "                                                           verbose=False),\n",
       "                                                  Index(['avg_dist', 'avg_rating_by_driver', 'avg_rating_of_driver', 'avg_surge',\n",
       "       'surge_pct', 'luxury_car_user', 'weekday_pct'],\n",
       "      dtype='object'))],\n",
       "                                   verbose=False)),\n",
       "                ('gaussiannb', GaussianNB(priors=None, var_smoothing=1e-09))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93552"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest 0.9592\n",
      "GaussianNB 0.93552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awantik/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.96024\n"
     ]
    }
   ],
   "source": [
    "for name,estimator in zip( ['RandomForest','GaussianNB', 'LogisticRegression'],[RandomForestClassifier(n_estimators=100), GaussianNB(), LogisticRegression()]):\n",
    "    pipeline = make_pipeline(preprocessor,estimator)\n",
    "    pipeline.fit(trainX,trainY)\n",
    "    print(name, pipeline.score(testX,testY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Furthur Fine Tuning\n",
    "* Feature Selection\n",
    "* Balancing Data\n",
    "* More Hyper-parameter Tuning\n",
    "* Consider month column from date-time cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SimpleImputer in module sklearn.impute._base:\n",
      "\n",
      "class SimpleImputer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin)\n",
      " |  SimpleImputer(missing_values=nan, strategy='mean', fill_value=None, verbose=0, copy=True, add_indicator=False)\n",
      " |  \n",
      " |  Imputation transformer for completing missing values.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <impute>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  missing_values : number, string, np.nan (default) or None\n",
      " |      The placeholder for the missing values. All occurrences of\n",
      " |      `missing_values` will be imputed.\n",
      " |  \n",
      " |  strategy : string, optional (default=\"mean\")\n",
      " |      The imputation strategy.\n",
      " |  \n",
      " |      - If \"mean\", then replace missing values using the mean along\n",
      " |        each column. Can only be used with numeric data.\n",
      " |      - If \"median\", then replace missing values using the median along\n",
      " |        each column. Can only be used with numeric data.\n",
      " |      - If \"most_frequent\", then replace missing using the most frequent\n",
      " |        value along each column. Can be used with strings or numeric data.\n",
      " |      - If \"constant\", then replace missing values with fill_value. Can be\n",
      " |        used with strings or numeric data.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |         strategy=\"constant\" for fixed value imputation.\n",
      " |  \n",
      " |  fill_value : string or numerical value, optional (default=None)\n",
      " |      When strategy == \"constant\", fill_value is used to replace all\n",
      " |      occurrences of missing_values.\n",
      " |      If left to the default, fill_value will be 0 when imputing numerical\n",
      " |      data and \"missing_value\" for strings or object data types.\n",
      " |  \n",
      " |  verbose : integer, optional (default=0)\n",
      " |      Controls the verbosity of the imputer.\n",
      " |  \n",
      " |  copy : boolean, optional (default=True)\n",
      " |      If True, a copy of X will be created. If False, imputation will\n",
      " |      be done in-place whenever possible. Note that, in the following cases,\n",
      " |      a new copy will always be made, even if `copy=False`:\n",
      " |  \n",
      " |      - If X is not an array of floating values;\n",
      " |      - If X is encoded as a CSR matrix;\n",
      " |      - If add_indicator=True.\n",
      " |  \n",
      " |  add_indicator : boolean, optional (default=False)\n",
      " |      If True, a `MissingIndicator` transform will stack onto output\n",
      " |      of the imputer's transform. This allows a predictive estimator\n",
      " |      to account for missingness despite imputation. If a feature has no\n",
      " |      missing values at fit/train time, the feature won't appear on\n",
      " |      the missing indicator even if there are missing values at\n",
      " |      transform/test time.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  statistics_ : array of shape (n_features,)\n",
      " |      The imputation fill value for each feature.\n",
      " |  \n",
      " |  indicator_ : :class:`sklearn.impute.MissingIndicator`\n",
      " |      Indicator used to add binary indicators for missing values.\n",
      " |      ``None`` if add_indicator is False.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  IterativeImputer : Multivariate imputation of missing values.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.impute import SimpleImputer\n",
      " |  >>> imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
      " |  >>> imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])\n",
      " |  ... # doctest: +NORMALIZE_WHITESPACE\n",
      " |  SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
      " |          missing_values=nan, strategy='mean', verbose=0)\n",
      " |  >>> X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]\n",
      " |  >>> print(imp_mean.transform(X))\n",
      " |  ... # doctest: +NORMALIZE_WHITESPACE\n",
      " |  [[ 7.   2.   3. ]\n",
      " |   [ 4.   3.5  6. ]\n",
      " |   [10.   3.5  9. ]]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  Columns which only contained missing values at `fit` are discarded upon\n",
      " |  `transform` if strategy is not \"constant\".\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SimpleImputer\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, missing_values=nan, strategy='mean', fill_value=None, verbose=0, copy=True, add_indicator=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fit the imputer on X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          Input data, where ``n_samples`` is the number of samples and\n",
      " |          ``n_features`` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : SimpleImputer\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Impute all missing values in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input data to complete.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numpy array of shape [n_samples, n_features]\n",
      " |          Training set.\n",
      " |      \n",
      " |      y : numpy array of shape [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      " |          Transformed array.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(SimpleImputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
